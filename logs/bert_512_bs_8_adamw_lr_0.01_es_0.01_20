2021-11-16 01:47:50,759 => 1.10.0
2021-11-16 01:47:50,760 => cuda
2021-11-16 01:47:51,275 => loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yinan/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
2021-11-16 01:48:03,158 => -------------------------------------------------
2021-11-16 01:48:03,158 => 1 FOLD
2021-11-16 01:48:03,791 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 01:48:03,792 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 01:48:04,192 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 01:48:08,283 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 01:48:08,283 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 01:51:40,276 => Train: 100.0	Epoch: 1	Iter: 158	Loss: 0.26681	Acc= 24.073	Precision= 24.122	Recall= 24.113	F1_score= 24.089
2021-11-16 01:55:07,365 => Train: 100.0	Epoch: 2	Iter: 158	Loss: 0.26437	Acc= 23.757	Precision= 23.758	Recall= 23.728	F1_score= 23.727
2021-11-16 01:58:55,907 => Train: 100.0	Epoch: 3	Iter: 158	Loss: 0.23858	Acc= 24.783	Precision= 24.702	Recall= 24.716	F1_score= 24.691
2021-11-16 02:02:23,772 => Train: 100.0	Epoch: 4	Iter: 158	Loss: 0.25136	Acc= 25.020	Precision= 24.989	Recall= 24.975	F1_score= 24.974
2021-11-16 02:06:29,005 => Train: 100.0	Epoch: 5	Iter: 158	Loss: 0.23068	Acc= 23.362	Precision= 23.323	Recall= 23.284	F1_score= 23.271
2021-11-16 02:09:56,409 => Train: 100.0	Epoch: 6	Iter: 158	Loss: 0.23163	Acc= 24.073	Precision= 23.995	Recall= 24.006	F1_score= 23.988
2021-11-16 02:13:23,378 => Train: 100.0	Epoch: 7	Iter: 158	Loss: 0.22377	Acc= 23.599	Precision= 23.509	Recall= 23.508	F1_score= 23.495
2021-11-16 02:16:50,479 => Train: 100.0	Epoch: 8	Iter: 158	Loss: 0.21462	Acc= 24.388	Precision= 24.242	Recall= 24.263	F1_score= 24.230
2021-11-16 02:20:17,232 => Train: 100.0	Epoch: 9	Iter: 158	Loss: 0.21528	Acc= 24.862	Precision= 24.730	Recall= 24.773	F1_score= 24.731
2021-11-16 02:23:43,953 => Train: 100.0	Epoch: 10	Iter: 158	Loss: 0.20679	Acc= 23.362	Precision= 23.250	Recall= 23.236	F1_score= 23.229
2021-11-16 02:27:11,581 => Train: 100.0	Epoch: 11	Iter: 158	Loss: 0.20498	Acc= 25.178	Precision= 24.986	Recall= 25.054	F1_score= 24.963
2021-11-16 02:31:46,342 => Train: 100.0	Epoch: 12	Iter: 158	Loss: 0.20158	Acc= 24.073	Precision= 23.997	Recall= 23.990	F1_score= 23.948
2021-11-16 02:37:14,182 => Train: 100.0	Epoch: 13	Iter: 158	Loss: 0.20230	Acc= 24.152	Precision= 24.048	Recall= 24.069	F1_score= 24.053
2021-11-16 02:40:42,223 => Train: 100.0	Epoch: 14	Iter: 158	Loss: 0.19706	Acc= 24.388	Precision= 24.399	Recall= 24.407	F1_score= 24.395
2021-11-16 02:45:39,656 => Train: 100.0	Epoch: 15	Iter: 158	Loss: 0.19314	Acc= 24.862	Precision= 24.811	Recall= 24.814	F1_score= 24.802
2021-11-16 02:49:10,680 => Train: 100.0	Epoch: 16	Iter: 158	Loss: 0.19375	Acc= 23.125	Precision= 23.134	Recall= 23.099	F1_score= 23.069
2021-11-16 02:54:25,086 => Train: 100.0	Epoch: 17	Iter: 158	Loss: 0.19177	Acc= 24.625	Precision= 24.506	Recall= 24.572	F1_score= 24.510
2021-11-16 02:57:52,774 => Train: 100.0	Epoch: 18	Iter: 158	Loss: 0.19135	Acc= 23.836	Precision= 23.592	Recall= 23.704	F1_score= 23.594
2021-11-16 03:01:24,756 => Train: 100.0	Epoch: 19	Iter: 158	Loss: 0.18919	Acc= 26.204	Precision= 26.183	Recall= 26.138	F1_score= 26.128
2021-11-16 03:04:52,380 => Train: 100.0	Epoch: 20	Iter: 158	Loss: 0.18728	Acc= 25.493	Precision= 25.482	Recall= 25.387	F1_score= 25.392
2021-11-16 03:08:25,722 => Train: 100.0	Epoch: 21	Iter: 158	Loss: 0.18610	Acc= 27.151	Precision= 27.084	Recall= 27.036	F1_score= 27.021
2021-11-16 03:11:55,672 => Train: 100.0	Epoch: 22	Iter: 158	Loss: 0.18571	Acc= 23.994	Precision= 23.921	Recall= 23.910	F1_score= 23.895
2021-11-16 03:17:09,310 => Train: 100.0	Epoch: 23	Iter: 158	Loss: 0.18402	Acc= 24.704	Precision= 24.638	Recall= 24.612	F1_score= 24.598
2021-11-16 03:20:38,633 => Train: 100.0	Epoch: 24	Iter: 158	Loss: 0.18325	Acc= 26.440	Precision= 26.189	Recall= 26.290	F1_score= 26.195
2021-11-16 03:24:10,150 => Train: 100.0	Epoch: 25	Iter: 158	Loss: 0.18196	Acc= 24.704	Precision= 24.457	Recall= 24.543	F1_score= 24.462
2021-11-16 03:27:42,226 => Train: 100.0	Epoch: 26	Iter: 158	Loss: 0.18320	Acc= 24.704	Precision= 24.640	Recall= 24.615	F1_score= 24.606
2021-11-16 03:31:15,116 => Train: 100.0	Epoch: 27	Iter: 158	Loss: 0.18326	Acc= 26.125	Precision= 25.822	Recall= 25.924	F1_score= 25.836
2021-11-16 03:34:53,952 => Train: 100.0	Epoch: 28	Iter: 158	Loss: 0.18238	Acc= 25.414	Precision= 25.311	Recall= 25.296	F1_score= 25.272
2021-11-16 03:38:25,257 => Train: 100.0	Epoch: 29	Iter: 158	Loss: 0.18187	Acc= 27.072	Precision= 26.895	Recall= 26.916	F1_score= 26.856
2021-11-16 03:41:54,505 => Train: 100.0	Epoch: 30	Iter: 158	Loss: 0.18166	Acc= 25.099	Precision= 24.654	Recall= 24.849	F1_score= 24.652
2021-11-16 03:45:22,449 => Train: 100.0	Epoch: 31	Iter: 158	Loss: 0.18100	Acc= 25.809	Precision= 25.653	Recall= 25.574	F1_score= 25.399
2021-11-16 03:48:56,609 => Train: 100.0	Epoch: 32	Iter: 158	Loss: 0.18034	Acc= 24.309	Precision= 23.949	Recall= 24.084	F1_score= 23.892
2021-11-16 03:52:27,683 => Train: 100.0	Epoch: 33	Iter: 158	Loss: 0.18019	Acc= 25.414	Precision= 25.568	Recall= 25.298	F1_score= 25.301
2021-11-16 03:56:03,436 => Train: 100.0	Epoch: 34	Iter: 158	Loss: 0.17950	Acc= 23.757	Precision= 23.581	Recall= 23.583	F1_score= 23.464
2021-11-16 03:59:37,497 => Train: 100.0	Epoch: 35	Iter: 158	Loss: 0.17983	Acc= 23.757	Precision= 23.407	Recall= 23.575	F1_score= 23.438
2021-11-16 04:03:12,047 => Train: 100.0	Epoch: 36	Iter: 158	Loss: 0.17972	Acc= 24.388	Precision= 24.087	Recall= 24.173	F1_score= 24.024
2021-11-16 04:06:42,619 => Train: 100.0	Epoch: 37	Iter: 158	Loss: 0.17999	Acc= 23.362	Precision= 22.968	Recall= 23.111	F1_score= 22.869
2021-11-16 04:10:11,795 => Train: 100.0	Epoch: 38	Iter: 158	Loss: 0.17930	Acc= 23.204	Precision= 23.023	Recall= 23.020	F1_score= 22.895
2021-11-16 04:13:43,440 => Train: 100.0	Epoch: 39	Iter: 158	Loss: 0.18004	Acc= 24.309	Precision= 23.897	Recall= 24.085	F1_score= 23.869
2021-11-16 04:17:16,855 => Train: 100.0	Epoch: 40	Iter: 158	Loss: 0.18016	Acc= 25.099	Precision= 25.127	Recall= 24.939	F1_score= 24.881
2021-11-16 04:22:33,494 => Train: 100.0	Epoch: 41	Iter: 158	Loss: 0.17848	Acc= 26.756	Precision= 26.218	Recall= 26.461	F1_score= 26.195
2021-11-16 04:26:03,509 => Train: 100.0	Epoch: 42	Iter: 158	Loss: 0.17947	Acc= 24.783	Precision= 24.609	Recall= 24.572	F1_score= 24.432
2021-11-16 04:29:40,550 => Train: 100.0	Epoch: 43	Iter: 158	Loss: 0.17721	Acc= 26.519	Precision= 26.247	Recall= 26.314	F1_score= 26.164
2021-11-16 04:33:09,690 => Train: 100.0	Epoch: 44	Iter: 158	Loss: 0.17679	Acc= 26.993	Precision= 26.485	Recall= 26.710	F1_score= 26.456
2021-11-16 04:36:36,562 => Train: 100.0	Epoch: 45	Iter: 158	Loss: 0.17952	Acc= 23.915	Precision= 23.760	Recall= 23.722	F1_score= 23.579
2021-11-16 04:40:07,639 => Train: 100.0	Epoch: 46	Iter: 158	Loss: 0.17722	Acc= 25.493	Precision= 25.176	Recall= 25.267	F1_score= 25.124
2021-11-16 04:43:36,086 => Train: 100.0	Epoch: 47	Iter: 158	Loss: 0.17843	Acc= 24.783	Precision= 24.578	Recall= 24.561	F1_score= 24.381
2021-11-16 04:47:07,994 => Train: 100.0	Epoch: 48	Iter: 158	Loss: 0.17868	Acc= 24.862	Precision= 24.606	Recall= 24.660	F1_score= 24.536
2021-11-16 04:50:41,968 => Train: 100.0	Epoch: 49	Iter: 158	Loss: 0.17912	Acc= 25.020	Precision= 24.767	Recall= 24.787	F1_score= 24.616
2021-11-16 04:54:15,035 => Train: 100.0	Epoch: 50	Iter: 158	Loss: 0.17760	Acc= 26.519	Precision= 26.256	Recall= 26.274	F1_score= 26.074
2021-11-16 04:54:18,421 => Eval: 100.0	Epoch: 50	Iter: 39	Loss: 0.17563	Acc= 23.659	Precision= 5.915	Recall= 25.000	F1_score= 9.566
2021-11-16 04:54:18,421 => -------------------------------------------------
2021-11-16 04:54:18,421 => 2 FOLD
2021-11-16 04:54:18,858 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 04:54:18,859 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 04:54:19,267 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 04:54:23,433 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 04:54:23,433 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 04:57:49,141 => Train: 100.0	Epoch: 1	Iter: 158	Loss: 0.27412	Acc= 24.388	Precision= 24.229	Recall= 24.357	F1_score= 24.260
2021-11-16 05:01:37,431 => Train: 100.0	Epoch: 2	Iter: 158	Loss: 0.28027	Acc= 24.625	Precision= 24.617	Recall= 24.632	F1_score= 24.571
2021-11-16 05:08:15,521 => Train: 100.0	Epoch: 3	Iter: 158	Loss: 0.26924	Acc= 24.862	Precision= 24.893	Recall= 24.869	F1_score= 24.867
2021-11-16 05:12:31,419 => Train: 100.0	Epoch: 4	Iter: 158	Loss: 0.24832	Acc= 24.941	Precision= 24.966	Recall= 24.963	F1_score= 24.939
2021-11-16 05:18:30,210 => Train: 100.0	Epoch: 5	Iter: 158	Loss: 0.23819	Acc= 24.941	Precision= 24.898	Recall= 24.919	F1_score= 24.888
2021-11-16 05:22:30,893 => Train: 100.0	Epoch: 6	Iter: 158	Loss: 0.24025	Acc= 24.467	Precision= 24.469	Recall= 24.447	F1_score= 24.430
2021-11-16 05:26:34,237 => Train: 100.0	Epoch: 7	Iter: 158	Loss: 0.22840	Acc= 23.678	Precision= 23.668	Recall= 23.665	F1_score= 23.655
2021-11-16 05:30:46,829 => Train: 100.0	Epoch: 8	Iter: 158	Loss: 0.22260	Acc= 22.889	Precision= 22.893	Recall= 22.863	F1_score= 22.865
2021-11-16 05:34:52,923 => Train: 100.0	Epoch: 9	Iter: 158	Loss: 0.21307	Acc= 23.047	Precision= 23.029	Recall= 23.050	F1_score= 23.023
2021-11-16 05:38:53,645 => Train: 100.0	Epoch: 10	Iter: 158	Loss: 0.21017	Acc= 23.757	Precision= 23.707	Recall= 23.729	F1_score= 23.703
2021-11-16 05:43:19,282 => Train: 100.0	Epoch: 11	Iter: 158	Loss: 0.20870	Acc= 24.546	Precision= 24.559	Recall= 24.559	F1_score= 24.559
2021-11-16 05:50:23,008 => Train: 100.0	Epoch: 12	Iter: 158	Loss: 0.20247	Acc= 23.047	Precision= 23.073	Recall= 23.045	F1_score= 23.045
2021-11-16 05:57:30,511 => Train: 100.0	Epoch: 13	Iter: 158	Loss: 0.20101	Acc= 23.204	Precision= 23.266	Recall= 23.218	F1_score= 23.224
2021-11-16 06:01:14,883 => Train: 100.0	Epoch: 14	Iter: 158	Loss: 0.19873	Acc= 23.678	Precision= 23.679	Recall= 23.692	F1_score= 23.679
2021-11-16 06:08:41,829 => Train: 100.0	Epoch: 15	Iter: 158	Loss: 0.19778	Acc= 22.573	Precision= 22.575	Recall= 22.587	F1_score= 22.560
2021-11-16 06:12:36,840 => Train: 100.0	Epoch: 16	Iter: 158	Loss: 0.19251	Acc= 24.625	Precision= 24.469	Recall= 24.651	F1_score= 24.497
2021-11-16 06:20:02,588 => Train: 100.0	Epoch: 17	Iter: 158	Loss: 0.18932	Acc= 25.099	Precision= 25.048	Recall= 25.117	F1_score= 24.994
2021-11-16 06:23:50,417 => Train: 100.0	Epoch: 18	Iter: 158	Loss: 0.18938	Acc= 24.388	Precision= 24.292	Recall= 24.422	F1_score= 24.297
2021-11-16 06:27:56,439 => Train: 100.0	Epoch: 19	Iter: 158	Loss: 0.18890	Acc= 24.546	Precision= 24.684	Recall= 24.561	F1_score= 24.529
2021-11-16 06:31:43,652 => Train: 100.0	Epoch: 20	Iter: 158	Loss: 0.18813	Acc= 25.414	Precision= 25.398	Recall= 25.407	F1_score= 25.321
2021-11-16 06:35:49,747 => Train: 100.0	Epoch: 21	Iter: 158	Loss: 0.18891	Acc= 23.599	Precision= 23.421	Recall= 23.603	F1_score= 23.438
2021-11-16 06:39:53,082 => Train: 100.0	Epoch: 22	Iter: 158	Loss: 0.18568	Acc= 24.388	Precision= 24.322	Recall= 24.425	F1_score= 24.340
2021-11-16 06:47:41,413 => Train: 100.0	Epoch: 23	Iter: 158	Loss: 0.18529	Acc= 23.204	Precision= 23.118	Recall= 23.178	F1_score= 23.107
2021-11-16 06:51:33,323 => Train: 100.0	Epoch: 24	Iter: 158	Loss: 0.18411	Acc= 23.757	Precision= 23.857	Recall= 23.753	F1_score= 23.686
2021-11-16 06:55:37,349 => Train: 100.0	Epoch: 25	Iter: 158	Loss: 0.18220	Acc= 24.388	Precision= 24.436	Recall= 24.372	F1_score= 24.380
2021-11-16 06:59:59,065 => Train: 100.0	Epoch: 26	Iter: 158	Loss: 0.18225	Acc= 25.414	Precision= 25.308	Recall= 25.387	F1_score= 25.303
2021-11-16 07:04:05,490 => Train: 100.0	Epoch: 27	Iter: 158	Loss: 0.18198	Acc= 24.388	Precision= 24.397	Recall= 24.368	F1_score= 24.306
2021-11-16 07:08:29,341 => Train: 100.0	Epoch: 28	Iter: 158	Loss: 0.18122	Acc= 25.414	Precision= 25.613	Recall= 25.405	F1_score= 25.448
2021-11-16 07:12:42,429 => Train: 100.0	Epoch: 29	Iter: 158	Loss: 0.18090	Acc= 24.467	Precision= 24.530	Recall= 24.440	F1_score= 24.397
2021-11-16 07:17:00,705 => Train: 100.0	Epoch: 30	Iter: 158	Loss: 0.17972	Acc= 26.125	Precision= 26.121	Recall= 26.093	F1_score= 26.068
2021-11-16 07:20:54,095 => Train: 100.0	Epoch: 31	Iter: 158	Loss: 0.17946	Acc= 25.335	Precision= 25.357	Recall= 25.298	F1_score= 25.256
2021-11-16 07:25:06,485 => Train: 100.0	Epoch: 32	Iter: 158	Loss: 0.17986	Acc= 25.572	Precision= 25.626	Recall= 25.545	F1_score= 25.539
2021-11-16 07:29:12,010 => Train: 100.0	Epoch: 33	Iter: 158	Loss: 0.18110	Acc= 25.493	Precision= 25.629	Recall= 25.498	F1_score= 25.509
2021-11-16 07:33:38,769 => Train: 100.0	Epoch: 34	Iter: 158	Loss: 0.17840	Acc= 27.466	Precision= 27.461	Recall= 27.415	F1_score= 27.344
2021-11-16 07:37:41,215 => Train: 100.0	Epoch: 35	Iter: 158	Loss: 0.17902	Acc= 26.361	Precision= 26.193	Recall= 26.289	F1_score= 26.170
2021-11-16 07:41:41,500 => Train: 100.0	Epoch: 36	Iter: 158	Loss: 0.17905	Acc= 25.730	Precision= 25.555	Recall= 25.674	F1_score= 25.547
2021-11-16 07:45:57,308 => Train: 100.0	Epoch: 37	Iter: 158	Loss: 0.18004	Acc= 24.625	Precision= 24.512	Recall= 24.580	F1_score= 24.494
2021-11-16 07:51:10,262 => Train: 100.0	Epoch: 38	Iter: 158	Loss: 0.18028	Acc= 23.757	Precision= 23.532	Recall= 23.697	F1_score= 23.539
2021-11-16 07:55:19,208 => Train: 100.0	Epoch: 39	Iter: 158	Loss: 0.18014	Acc= 23.678	Precision= 23.431	Recall= 23.591	F1_score= 23.442
2021-11-16 07:59:31,662 => Train: 100.0	Epoch: 40	Iter: 158	Loss: 0.17928	Acc= 25.099	Precision= 25.057	Recall= 25.052	F1_score= 24.974
2021-11-16 08:09:49,225 => Train: 100.0	Epoch: 41	Iter: 158	Loss: 0.17971	Acc= 24.388	Precision= 24.366	Recall= 24.325	F1_score= 24.245
2021-11-16 08:14:54,716 => Train: 100.0	Epoch: 42	Iter: 158	Loss: 0.17946	Acc= 24.941	Precision= 24.936	Recall= 24.907	F1_score= 24.877
2021-11-16 08:21:20,015 => Train: 100.0	Epoch: 43	Iter: 158	Loss: 0.17888	Acc= 23.520	Precision= 23.533	Recall= 23.465	F1_score= 23.419
2021-11-16 08:26:22,919 => Train: 100.0	Epoch: 44	Iter: 158	Loss: 0.17852	Acc= 24.546	Precision= 24.438	Recall= 24.481	F1_score= 24.386
2021-11-16 08:31:27,444 => Train: 100.0	Epoch: 45	Iter: 158	Loss: 0.17702	Acc= 25.335	Precision= 25.236	Recall= 25.289	F1_score= 25.230
2021-11-16 08:37:49,928 => Train: 100.0	Epoch: 46	Iter: 158	Loss: 0.17994	Acc= 24.309	Precision= 24.146	Recall= 24.235	F1_score= 24.114
2021-11-16 08:42:50,459 => Train: 100.0	Epoch: 47	Iter: 158	Loss: 0.18010	Acc= 22.494	Precision= 22.337	Recall= 22.442	F1_score= 22.341
2021-11-16 08:48:47,838 => Train: 100.0	Epoch: 48	Iter: 158	Loss: 0.17792	Acc= 25.967	Precision= 25.933	Recall= 25.908	F1_score= 25.856
2021-11-16 08:53:51,000 => Train: 100.0	Epoch: 49	Iter: 158	Loss: 0.17920	Acc= 23.362	Precision= 23.345	Recall= 23.303	F1_score= 23.208
2021-11-16 09:00:29,597 => Train: 100.0	Epoch: 50	Iter: 158	Loss: 0.17732	Acc= 25.967	Precision= 25.962	Recall= 25.938	F1_score= 25.877
2021-11-16 09:00:32,985 => Eval: 100.0	Epoch: 50	Iter: 39	Loss: 0.17501	Acc= 26.498	Precision= 6.625	Recall= 25.000	F1_score= 10.474
2021-11-16 09:00:32,985 => -------------------------------------------------
2021-11-16 09:00:32,985 => 3 FOLD
2021-11-16 09:00:33,618 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 09:00:33,619 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 09:00:34,221 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 09:00:38,036 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 09:00:38,036 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 09:07:19,333 => 1.10.0
2021-11-16 09:07:19,347 => cuda
2021-11-16 09:07:20,283 => loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yinan/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
2021-11-16 09:07:32,174 => -------------------------------------------------
2021-11-16 09:07:32,174 => 1 FOLD
2021-11-16 09:07:32,575 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 09:07:32,577 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 09:07:32,962 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 09:07:36,845 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 09:07:36,845 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 09:12:48,474 => 1.10.0
2021-11-16 09:12:48,475 => cuda
2021-11-16 09:12:48,901 => loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yinan/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
2021-11-16 09:13:00,906 => -------------------------------------------------
2021-11-16 09:13:00,906 => 1 FOLD
2021-11-16 09:13:01,310 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 09:13:01,312 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 09:13:01,749 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 09:13:05,644 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 09:13:05,644 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 09:13:16,227 => 1.10.0
2021-11-16 09:13:16,227 => cuda
2021-11-16 09:13:16,642 => loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yinan/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
2021-11-16 09:13:28,590 => -------------------------------------------------
2021-11-16 09:13:28,590 => 1 FOLD
2021-11-16 09:13:29,000 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 09:13:29,002 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 09:13:29,401 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 09:13:33,071 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 09:13:33,072 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 09:13:44,249 => 1.10.0
2021-11-16 09:13:44,249 => cuda
2021-11-16 09:13:44,678 => loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yinan/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
2021-11-16 09:13:56,636 => -------------------------------------------------
2021-11-16 09:13:56,636 => 1 FOLD
2021-11-16 09:13:57,046 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 09:13:57,048 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 09:13:57,462 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 09:14:01,118 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 09:14:01,118 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
2021-11-16 09:14:12,968 => 1.10.0
2021-11-16 09:14:12,968 => cuda
2021-11-16 09:14:13,375 => loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/yinan/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1
2021-11-16 09:14:25,343 => -------------------------------------------------
2021-11-16 09:14:25,343 => 1 FOLD
2021-11-16 09:14:25,753 => loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/yinan/.cache/torch/transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391
2021-11-16 09:14:25,754 => Model config BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": null,
  "do_sample": false,
  "eos_token_ids": null,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_decoder": false,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1
  },
  "layer_norm_eps": 1e-12,
  "length_penalty": 1.0,
  "max_length": 20,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_beams": 1,
  "num_hidden_layers": 12,
  "num_labels": 4,
  "num_return_sequences": 1,
  "output_attentions": false,
  "output_hidden_states": false,
  "output_past": true,
  "pad_token_id": 0,
  "pruned_heads": {},
  "repetition_penalty": 1.0,
  "temperature": 1.0,
  "top_k": 50,
  "top_p": 1.0,
  "torchscript": false,
  "type_vocab_size": 2,
  "use_bfloat16": false,
  "vocab_size": 28996
}

2021-11-16 09:14:26,163 => loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/yinan/.cache/torch/transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2
2021-11-16 09:14:29,808 => Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']
2021-11-16 09:14:29,808 => Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
